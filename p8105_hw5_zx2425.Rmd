---
title: "p8105_hw5_zx2425"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(htmlwidgets)
library(flexdashboard)
library(p8105.datasets)
library(rvest)
library(readxl)
```
## Problem 1

## step1
The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

## step2
```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

## step3
Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

# Problem2
## step1
Describe the raw data. Create a city_state variable (e.g. “Baltimore, MD”) 

```{r}
hmcd =read.csv("./homicide-data.csv") %>% 
  janitor::clean_names() 
head(hmcd)

```
The data set has `r nrow(hmcd)` * ` r ncol(hmcd)` dimensions. Where uid describes the incident number, reported_date indicates the date and time, victim_last and victim first indicates the victim's name, and other data includes: race, age, gender, city, state, longitude, and latitude, and whether the incident was finally detected.

## step2
then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
hmcd=hmcd %>% 
  mutate(
    city_state=str_c(city,"_",state)
  )
hmcd= hmcd %>% 
  filter(city_state !="Tulsa_AL")

count_vic=hmcd %>% 
  group_by(city_state) %>% 
  summarize(
    vic_number=n()
  )
count_dect=hmcd %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved_number=n()
  )
  

count = full_join(count_vic,count_dect,by="city_state")
count[is.na(count)] = 0
count %>% 
  arrange(desc(vic_number))
count
```
We can see from the table that vic_number represents the number of crimes that occurred in the area, and unsolved_number represents the number of unsolved cases in the area.
By the way, because in further step, we will find that the "Tulsa_AL" is extreme data with vic_number of 1 and unresolved number of 0. It will definitely influence our further exploration, so we will exclude it from now on. 

## step3
For the city of __Baltimore, MD__, use the __prop.test__ function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the __broom::tidy__ to this object and pull the _estimated_ _proportion_ and _confidence intervals_ from the resulting tidy dataframe.

```{r}
city_B= hmcd %>% 
  filter(city_state == 'Baltimore_MD')
city_B_count= city_B %>% 
   summarise(
      unsolved = sum(disposition %in%c("Closed without arrest", "Open/No arrest")  ),
      ###sum can calculate the True or false with result of number
      n = n()
    )
city_B_test = 
  prop.test(x = city_B_count$unsolved , n = city_B_count$n)
city_B_test %>% 
  broom::tidy() %>%
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)  

```
## step4
Now run __prop.test__ for _each of the cities_ in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of __purrr::map, purrr::map2__, list columns and __unnest__ as necessary to create a tidy dataframe with _estimated proportions and CIs_ for each city.

```{r}
pro_test_df= function(each_city){
  city_info=
    each_city %>% 
    summarize(
      unsolved = sum(disposition %in%c("Closed without arrest", "Open/No arrest")  ),
      n=n()
    )
  city_test= prop.test(
    x=city_info %>% pull(unsolved),
    n=city_info %>% pull(n)
  )
  city_test
}
```

```{r}
city_iterate = 
  hmcd %>% 
    nest(-city_state) %>% 
    mutate(
      test_results = map(data, pro_test_df),
      tidy_results = map(test_results, broom::tidy)
    ) %>% 
    select(city_state, tidy_results) %>% 
    unnest(tidy_results) %>%
    select(city_state, estimate, starts_with('conf'))
city_iterate
```
## step5
Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.
```{r}
city_iterate %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))
```
By observing the plote, we can see there is a extreme data which is Tulsa_AL, we delete it can replot again.
```{r}
city_iterate = city_iterate %>% 
  filter(city_state !="Tulsa_AL")
city_iterate %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate,.desc=F)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimates and CIs for each city",
    x = "City_State",
    y = "Proportion") +
  theme(axis.text.x = element_text(angle = 90))
```
From the figure above, we can observe estimate for proportion of each city.

# problem3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

# step1
Let's define a t.test function with known parameter
```{r}
t_test  = function(mu, n=30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
   t_result = t.test(sim_data,mu=0) %>% 
    broom::tidy() %>% 
    select(estimate,p.value)
  t_result
}

```
## Step=2
Let's simulate when u=0
```{r}
t_test(0)
sim_results_df = 
  expand_grid(
    mean_1 = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    t_result = map(mean_1,t_test)
  ) %>% 
  unnest(t_result)

```

## step=3
Let's simulate when u=1:6
```{r}
mean_16 = expand_grid(mean_16 = 1:6, iteration = 1:5000) %>% 
  mutate(t_result = map(mean_16,t_test))
mean_16result = mean_16 %>%   
unnest(t_result)
mean_16result
mean_16result= na.omit(mean_16result) 
```
## step4
Make a plot showing the __proportion of times the null was rejected__ (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
Let's us observe when u is equal to different value, the proportion of times that we reject H0
```{r}
plot1_df=mean_16result %>% 
  group_by(mean_16) %>% 
  summarize(
    reject_case=sum(p.value<0.05),
    ##sum the total number of rejectedcase
    reject_proportion=reject_case/5000
  )

plot1=plot1_df %>% 
  ggplot(aes(x=mean_16,y=reject_proportion))+
    scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11))+
      geom_point()+
      geom_path()+
    labs(
    title = "proportion - mu ",
    x = "mu",
    y = "rejected_Proportion")
  
plot1
```

With the effect size and true value of mu that we set, increase, the reject proportion increase at the same time. Furthermore, the trend of the value change is tend to be 1.This is because when the actual value of mu is increase, the fault of the test make will be less and less. As a result, the trend of proportion will tend to be 1

## step5
Make a plot showing the average estimate of μ^ on the y axis and the true value of μ on the x axis. 
```{r}
full_eti_u=mean_16result %>%
  group_by(mean_16) %>% 
  summarize(avg_eti = mean(estimate)) 

 plot_a=full_eti_u%>% 
  ggplot(aes(x = mean_16,y = avg_eti)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) + 
  geom_point() + 
  geom_path()+
  labs(
    title = "avg_estimu vs Ture_mu",
    x = "True_mu",
    y = "avg_estimu") 
 plot_a
```

## step 6
Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
```{r}
rej_eti_u = mean_16result %>% 
  filter(p.value < 0.05) %>% 
  group_by(mean_16) %>% 
  summarize(
    avg_eti = mean(estimate))

plot2=rej_eti_u %>% 
  ggplot(aes(x = mean_16,y = avg_eti)) +
  scale_x_continuous(limits = c(1,6), breaks = seq(1,6,1)) + 
  geom_point() + 
  geom_path()+
  labs(
    title = "r_avg_estimu vs Ture_mu",
    x = "True_mu",
    y = "r_avg_estimu")
plot2

## here is to recaculate some variables
mean_16result %>%
  group_by(mean_16) %>% 
  summarize(avg_eti = mean(estimate)) %>% 
  ggplot(aes(x = mean_16,y = avg_eti)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) + 
  geom_point() + 
  geom_path()+
  labs(
    title = "avg_estimu vs True_mu",
    x = "True_mu",
    y = "avg_estimu")

```

```{r}

ggplot(full_eti_u,aes(x = mean_16, y = avg_eti)) +
geom_line(data = full_eti_u,aes(colour = "green")) +
geom_line(data = rej_eti_u,aes(colour = "red")) +
scale_color_manual(name = " ", values = c("green" = "green", "red" = "red"),
labels = c('Est_all','Est_rej'))+
geom_point(data = full_eti_u,colour = "blue") +
geom_point(data = rej_eti_u,colour = "black")

```

From the plot we can see, the line is combined after mu>=4. and the average value of rejected estimates data set is larger than the all estimates data set when mmu<4.
This shows that before mu is less than or equal to 4, there is an error in the t.test test, so some points in the normal distribution data set generated by the mean value under the assumption that u is not equal to H0 will fall within the acceptance threshold, making the rejection estimates data set's sample mean higher than all estimates datasets.
